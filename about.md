---
layout: about
title: about
permalink: /
subtitle: >
  <a href='https://www.cbl-cambridge.org/people/gf332'>Imperial College Research Fellow</a>, Imperial College London.
cv_pdf: 'MLcvMinimal/CV_Gergely_Flamich.pdf'

profile:
  align: right
  image: profile_pic_3_cropped.jpeg
  image_circular: false # crops the image to make it circular
  address: >
    <p>Information Processing and Communications Lab,</p>
    <p>Department of Electrical and Electronic Engineering,</p>
    <p>Imperial College London,</p>
    <p>South Kensington Campus, London, UK</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

Hello there! I'm Gergely Flamich (pronounced <span style="font-style:italic;font-variant:small-caps; font-weight:400">gher - gey flah - mih</span> but in English, I usually go by Greg), and I'm originally from Vác, Hungary. 

From 1 August 2025, I am an [Imperial College Research Fellow](https://www.imperial.ac.uk/research-and-innovation/research-office/funder-information/research-fellowships/icrf/), sponsored by [Deniz Gündüz](https://profiles.imperial.ac.uk/d.gunduz) and hosted at the [Information Processing and Communications Lab](https://www.imperial.ac.uk/information-processing-and-communications-lab/) at Imperial College London.
Before that, I spent five months as a postdoctoral research associate in the same lab.

I completed my PhD degree between October 2020 and December 2024 in Advanced Machine Learning at the [Computational and Biological Learning Lab](https://www.cbl-cambridge.org/), supervised by [José Miguel Hernández Lobato](https://jmhl.org/). 
I also hold an MPhil degree in [Machine Learning and Machine Intelligence](https://www.mlmi.eng.cam.ac.uk/) from the University of Cambridge and a Joint BSc Honours degree in Mathematics and Computer Science from the University of St Andrews.

During the winter of 2024-2025, I worked with [David Vilar](https://research.google/people/davidvilar/?&type=google), [Jan-Thorsten Peter](https://scholar.google.com/citations?user=pUS4FG4AAAAJ&hl=en) and [Markus Freitag](https://freitagmarkus.github.io/) as a Student Researcher at Google Berlin. We worked on generative models for neural machine translation and we developed the theory of [the accuracy-naturalness tradeoff in translation](https://arxiv.org/abs/2503.24013).

From July 2022 until Dec 2022, I worked with [Lucas Theis](http://theis.io/) as a Student Researcher at Google Brain, during which time we developed [adaptive greedy rejection sampling and bits-back quantization](https://arxiv.org/abs/2304.10407).

My research focuses on the theory of [relative entropy coding/channel simulation](https://www.repository.cam.ac.uk/items/22b78b91-ce53-44cb-86ac-83d812888aec) and its application to neural data compression. 
Relative entropy coding algorithms allow us to efficiently encode a random sample from both discrete and continuous distributions, and they are a natural alternative to quantization and entropy coding in lossy compression codecs.
Furthermore, they bring unique advantages to lossy compression once we go beyond the standard rate-distortion framework: they allow us to design optimally efficient [artefact-free/perfectly realistic lossy compression](https://arxiv.org/abs/2206.08889) codecs using generative models and perform [differentially private federated learning](https://arxiv.org/abs/2111.00092) with optimal communication cost.
Unfortunately, relative entropy coding hasn't seen widespread adoption, as all current algorithms are either too slow or have limited applicability.

Hence, I am focusing on addressing this issue by developing fast, general-purpose coding algorithms, such as [A* coding](https://arxiv.org/abs/2201.12857) and [greedy Poisson rejection sampling](https://arxiv.org/abs/2305.15313), and providing mathematical guarantees on their coding efficiency and runtime.
In addition to my theoretical work, I am also interested in applying relative entropy coding algorithms to neural compression, utilizing generative models such as [variational autoencoders](https://arxiv.org/abs/2010.01185) and [implicit neural representations](https://arxiv.org/abs/2305.19185).


Judit, my wonderful sister, is an amazing painter. 
Check out her work on [Instagram](https://www.instagram.com/j.flamich.art/)!