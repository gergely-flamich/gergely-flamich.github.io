#+TITLE: An Introduction to Relative Entropy Coding and Its Applications
#+author: Gergely Flamich
#+date: 31/07/2024

#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
# This is needed to make the speaker notes work
#+REVEAL_REVEAL_JS_VERSION: 4
#+OPTIONS: reveal_title_slide:"<h2>%t</h2><h2>%s</h2></br><h4>%a</h4><h4>%d</h4><h6>gergely-flamich.github.io</h6>"
#+OPTIONS: toc:nil
#+OPTIONS: num:nil
#+REVEAL_THEME: white
#+REVEAL_INIT_OPTIONS: slideNumber:'c/t', transition:'none'
#+REVEAL_HLEVEL:0
#+REVEAL_MATHJAX_URL: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
#+REVEAL_EXTRA_CSS: ./presentation_styles.css

* In Collaboration With

#+REVEAL_HTML: <img src="./img/collaborators/jiajun_he.jpg" width=23% >
#+REVEAL_HTML: <img src="./img/collaborators/zongyu_guo.jpg" width=23%>
#+REVEAL_HTML: <img src="./img/collaborators/daniel_goc.jpg" width=23%>
#+REVEAL_HTML: <img src="./img/collaborators/lennie_wells.jpg" width=23%>
#+REVEAL_HTML: <img src="./img/collaborators/stratis_markou.jpg" width=23%>
#+REVEAL_HTML: <img src="./img/collaborators/miguel_hernandez_lobato.png" width=23%>

* why care?

** transform coding

[[./img/transform_coding.png]]

** realistic lossy compression
#+REVEAL_HTML: <img src="./img/classic_vs_gen_compression.png" width=80% >

Right-hand image from Careil et al. [1]

** differentially private federated learning
#+REVEAL_HTML: <img src="./img/federated_learning_sketch.png" width=100% >

** relative entropy coding
#+ATTR_REVEAL: :frag (appear)
1. $X, Y \sim P_{X, Y}$
2. Given $X \sim P_X$, encoder sends code $C$ so that decoder can compute $Y \sim P_{Y \mid X}$
3. Li and El Gamal [2]:
#+ATTR_REVEAL: :frag (appear)
$$
{\color{red} I[X; Y]} \leq \mathbb{E}[|C|] \leq {\color{red} I[X; Y]} + {\color{blue} \log (I[X; Y] + 1) + 4}
$$
#+REVEAL_HTML: <div class="tick-list">
#+ATTR_REVEAL: :frag (appear)
- short codelength
#+REVEAL_HTML: </div>

** lossy source coding
#+ATTR_REVEAL: :frag (appear)
*Usually:*
#+ATTR_REVEAL: :frag (appear)
- $f^{-1} \circ \lfloor \cdot \rceil \circ f$
- $f$ controls distortion

#+ATTR_REVEAL: :frag (appear)
*Now:*
#+ATTR_REVEAL: :frag (appear)
- $f^{-1} \circ (f + \epsilon)$
- $f$ controls $P_{Y \mid X}$

** common randomness
#+REVEAL_HTML: <img src="./img/common_randomness_meme.jpg" width=55% class="fragment (appear)">

** solution 1: dither
#+ATTR_REVEAL: :frag (appear)
*Common randomness:* $U \sim \mathrm{Unif}[-1/2, 1/2]$

#+ATTR_REVEAL: :frag (appear)
\[
\lfloor x + U \rceil - U \sim  \mathrm{Unif}[x - 1/2, x + 1/2]
\]

#+REVEAL_HTML: <div class="invis-list">
#+ATTR_REVEAL: :frag (appear)
- *Encoder:* receive $X \sim P_X$, send $K = \lfloor X + U\rceil$
- *Decoder:* compute $Y = K - U$
-
- ‚úÖ Fast
- ‚ùå Can only simulate uniform distributions.
#+REVEAL_HTML: </div>

** solution 2: selection sampling
#+REVEAL_HTML: <img src="./img/selection_sampling_sketch.png" width=100% class="fragment (appear)">
#+REVEAL_HTML: <div class="invis-list">
#+ATTR_REVEAL: :frag (appear)
- ‚úÖ Can simulate any distribution
- ‚ùå Slow
#+REVEAL_HTML: </div>

# #+ATTR_REVEAL: :frag (appear)
# *Common randomness:* $X_1, X_2, \dots, \sim P_Y$
# 
# #+ATTR_REVEAL: :frag (appear)
# *Alice:* Gets $X \sim P_X$, select $X_N$, send $N$.
# 
# #+ATTR_REVEAL: :frag (appear)
# *Bob:* Simulate $X_N \sim P_{Y \mid X}$.


* Greedy Poisson Rejection Sampling
#+ATTR_REVEAL: :frag (appear)
- Sampling as search
- Exploit structure to accelerate search
- $Q \gets P_{Y \mid X}$
- $P \gets P_{Y}$

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_0.png]]

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_1.png]]

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_2.png]]

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_3.png]]

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_4.png]]

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_5.png]]

** GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_accept.png]]


** Fast GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_0.png]]
** Fast GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_1.png]]
** Fast GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_2.png]]
** Fast GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_3.png]]
** Fast GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_4.png]]
** Fast GPRS with $P = \mathcal{N}(0, 1), Q = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_5.png]]


** Analysis of faster GPRS
#+ATTR_REVEAL: :frag (appear)
Now, encode search path $\pi$.

#+ATTR_REVEAL: :frag (appear)
$\mathbb{H}[\pi] \leq I[X; Y] + \log(I[X; Y] + 1) + \mathcal{O}(1)$

#+ATTR_REVEAL: :frag (appear)
$\mathbb{E}[\lvert\pi\rvert] = I[X; Y] + \mathcal{O}(1)$

#+ATTR_REVEAL: :frag (appear)
This is *optimal*.


* Computationally Lightweight ML-based data compression
** Data Compression with INRs
#+REVEAL_HTML: <img src="./img/applications/coin.png" class="r-stretch">
Image from Dupont et al. [4]

#+REVEAL_HTML: <div class="tick-list">
#+ATTR_REVEAL: :frag (appear)
- computationally lightweight
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div class="cross-list">
#+ATTR_REVEAL: :frag (appear)
- short codelength
#+REVEAL_HTML: </div>

** COMBINER

COMpression with Bayesian Implicit Neural Representations

#+REVEAL_HTML: <img src="./img/applications/variational_bnn.png" class="r-stretch">
Image from Blundell et al. [7]

#+ATTR_REVEAL: :frag (appear)
üí°Gradient descent is the transform!

** COMBINER
#+REVEAL_HTML: <img src="./img/applications/recombiner_img.png" width="100%">

** COMBINER
#+REVEAL_HTML: <img src="./img/applications/recombiner.png" width="100%">

* Theory: What next?
#+ATTR_REVEAL: :frag (appear)
 - Might not need perfect solution: think of error correcting codes (e.g. LDPC)
 - Exploit different types of structure
 - Duality between source and channel coding

* Applications: What next?
#+ATTR_REVEAL: :frag (appear)
 - Realism constraints for INR-based compression
 - Scale to high-resolution/high-volume data

* Take-home messages
#+ATTR_REVEAL: :frag (appear)
 - *Relative entropy coding* is a stochastic alternative to quantization for lossy source coding
 - Currently two flavours: *dither*-based, and *selection sampling*
 - *Greedy Poisson rejection sampling* is an optimal selection sampler
 - *Implicit neural represenations* are an exciting, *compute-efficient* approach to data compression with huge potential

* References I
 - [1] Careil, M., Muckley, M. J., Verbeek, J., & Lathuili√®re, S. Towards image compression with perfect realism at ultra-low bitrates. ICLR 2024.
 - [2] C. T. Li and A. El Gamal, ‚ÄúStrong functional representation lemma and applications to coding theorems,‚Äù IEEE Transactions on Information Theory, vol. 64, no. 11, pp. 6967‚Äì6978, 2018.
 - [3] E. Agustsson and L. Theis. "Universally quantized neural compression" In NeurIPS 2020.

* References II
 - [4] E. Dupont, A. Golinski, M. Alizadeh, Y. W. Teh and Arnaud Doucet. "COIN: compression with implicit neural representations" arXiv preprint arXiv:2103.03123, 2021.
 - [5] G. F., L. Wells, Some Notes on the Sample Complexity of Approximate Channel Simulation. To appear at Learning to Compress workshop @ ISIT 2024.
 - [6] D. Goc, G. F. On Channel Simulation with Causal Rejection Samplers. To appear at ISIT 2024

* References III
 - [7] C. Blundell, J. Cornebise, K. Kavukcuoglu and D. Wierstra. Weight uncertainty in neural network. In ICML 2015.
