#+TITLE: Relative Entropy Coding with Greedy Poisson Rejection Sampling
#+author: Gergely Flamich
#+date: 29/09/2023

#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+OPTIONS: reveal_title_slide:"<h2>%t</h2><h2>%s</h2></br><h4>%a</h4><h4>%d</h4>"
#+OPTIONS: toc:nil
#+REVEAL_THEME: white
#+REVEAL_INIT_OPTIONS: slideNumber:'c/t', transition:'none'
#+REVEAL_HLEVEL:0
#+REVEAL_MATHJAX_URL: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
# https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js
# https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js

* In Collaboration With

#+REVEAL_HTML: <img src="./img/collaborators/jiajun_he.jpg" width=23% >
#+REVEAL_HTML: <img src="./img/collaborators/zongyu_guo.jpg" width=23%>
#+REVEAL_HTML: <img src="./img/collaborators/daniel_goc.jpg" width=23%>
#+REVEAL_HTML: <img src="./img/collaborators/miguel_hernandez_lobato.png" width=23%>

* What is Relative Entropy Coding?

** Lossy compression
#+ATTR_REVEAL: :frag (appear)
[[./img/jpeg_example/transform_encoding.png]]

#+ATTR_REVEAL: :frag (appear)
[[./img/jpeg_example/transform_decoding.png]]


** Transform Coding as a Channel
#+ATTR_REVEAL: :frag (appear)
Get an image $Y \sim P_Y$

#+ATTR_REVEAL: :frag (appear)
Transform and quantize: $X = f(Y)$

#+ATTR_REVEAL: :frag (appear)
Inverse transform to reconstruct: $\hat{Y} = g(X)$

#+ATTR_REVEAL: :frag (appear)
What can we say about $P_{X \mid Y}$? Could we control it?


** Relative Entropy Coding/Channel Simulation
#+ATTR_REVEAL: :frag (appear)
Correlated r.v.s $X, Y \sim P_{X, Y}$

#+ATTR_REVEAL: :frag (appear)
Alice receives $Y \sim P_Y$

#+ATTR_REVEAL: :frag (appear)
Bob wants to simulate $X \sim P_{X \mid Y}$

#+ATTR_REVEAL: :frag (appear)
How many bits does Alice need to send to Bob?

** Relative Entropy Coding/Channel Simulation

#+ATTR_REVEAL: :frag (appear)
How many bits does Alice need to send to Bob?

#+ATTR_REVEAL: :frag (appear)
When common randomness $S$ available:
$$
H[X \mid S] \leq I[X; Y] + \log (I[X; Y] + 1) + 4.
$$

#+ATTR_REVEAL: :frag (appear)
$H[X]$ - Amount of information in $P_X$

#+ATTR_REVEAL: :frag (appear)
$I[X; Y]$ - Information in $P_{X, Y}$ compared to $P_X \otimes P_Y$

#+ATTR_REVEAL: :frag (appear)
$I[X; Y]$ can be *finite* even when $H[X]$ is *infinite*!

* Applications of Relative Entropy Coding

** Lossy Compression with Realism Constraints
# +ATTR_REVEAL: :frag (appear)
- Theis & Agustsson (2021):
  - REC provably better than quantization.
- Theis et al. (2022):
#+REVEAL_HTML: <img src="./img/applications/diffC.png" class="r-stretch" data-transition="appear">

** Model Compression
#+REVEAL_HTML: <img src="./img/applications/variational_bnn.png" class="r-stretch">
# +ATTR_REVEAL: :frag (appear)
# - Dataset $\mathcal{D} \sim P_{\mathcal{D}}$
# - NN $f(w, x)$ with weights $w$ with prior $P_w$
# - Train weight posterior $P_{w \mid \mathcal{D}}$ using ELBO
# - Encode $w \sim P_{w \mid \mathcal{D}}$ in $I[w; \mathcal{D}]$ bits

#+ATTR_REVEAL: :frag (appear)
Image from Blundell et al. (2015)

** Model Compression
Havasi et al. (2018): MIRACLE
#+REVEAL_HTML: <img src="./img/applications/miracle.png" class="r-stretch">

** Data Compression with INRs
#+REVEAL_HTML: <img src="./img/applications/coin.png" class="r-stretch">
Image from Dupont et al. (2021)

#+ATTR_REVEAL: :frag (appear)
*Problem*: Post-training quantization severely impacts performance!

** Compress variational INRs!
*COMBINER*: COMpression with Bayesian Implicit Neural Representations

#+REVEAL_HTML: <img src="./img/applications/recombiner.png" width="100%">
# +REVEAL_HTML: <section>
# +REVEAL_HTML: <img src="./img/applications/combiner/psnr_kodak.png" width="45%">
# +REVEAL_HTML: <img src="./img/applications/combiner/psnr_audio.png" width="45%">
# +REVEAL_HTML: </section>

* Current limitations of REC
#+ATTR_REVEAL: :frag (appear)
Current REC algorithms are:
#+ATTR_REVEAL: :frag (appear)
- Too slow (Agustsson & Theis, 2020):
  - Average runtime of any general REC algorithm must scale at least $2^{I[X; Z]}$
- Too limited:
  - Uniforms only (Agustsson & Theis, 2020)
  - 1D unimodal distributions only (F et al., 2022)
- Too much codelength overhead

#+ATTR_REVEAL: :frag (appear)
*Open problem:* $\mathcal{O}(I[X; Z])$ runtime when both $P_{Z \mid X}$ and $P_Z$ are multivariate Gaussian?

* Take home message: Overview and Applications
#+ATTR_REVEAL: :frag (appear)
- REC is a stochastic compression framework
- Alternative to quantization and entropy coding
- It finds applications in:
  - Lossy compression with realism constraints
  - Model compression
  - Compressing Bayesian INRs
- Currently still too slow or limited


* Greedy Poisson Rejection Sampling
** Poisson Processes
#+ATTR_REVEAL: :frag (appear)
 - Collection of random points in space
 - Focus on spatio-temporal processes on $\mathbb{R}^D \times \mathbb{R}^+$
 - Exponential inter-arrival times
 - Spatial distribution $P_{X}$

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/empty_pp.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_t1.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_x1.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_t1_x1.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_t2.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_x2.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_t2_x2.png]]

** Example with $P_{X} = \mathcal{N}(0, 1)$
[[./img/pp/pp_sim.png]]


** Greedy Poisson Rejection Sampling

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_0.png]]

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_1.png]]

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_2.png]]

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_3.png]]

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_4.png]]

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_5.png]]

** GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/gprs/gprs_accept.png]]

** How to find the function?

#+ATTR_REVEAL: :frag (appear)
$$
\sigma(h) = \int_0^h \frac{1}{w_{X \mid Y}(\eta) - \eta \cdot w_X(\eta)} \, d\eta,
$$
#+ATTR_REVEAL: :frag (appear)
where
$$
w_X(h) = \mathbb{P}_{Z \sim P_X}\left[\frac{dP_{X \mid Y}}{dP_X}(Z) \geq h \right]
$$
$$
w_{X \mid Y}(h) = \mathbb{P}_{Z \sim P_{X \mid Y}}\left[\frac{dP_{X \mid Y}}{dP_X}(Z) \geq h \right]
$$

** Analysis of GPRS
#+ATTR_REVEAL: :frag (appear)
*Codelength*
#+ATTR_REVEAL: :frag (appear)
#+ATTR_REVEAL: :frag (appear)
\begin{align}
H[X \mid S] &\leq I[X; Y] + \log (I[X; Y] + 1) \\
&\quad + 2 + \frac{1}{1 + I[X; Y] \cdot \ln 2}
\end{align}

#+ATTR_REVEAL: :frag (appear)
*Runtime*

#+ATTR_REVEAL: :frag (appear)
$$
\mathbb{E}[K \mid Y] = \exp(D_{\inf}[P_{X \mid Y} \Vert P_X])
$$

** Speeding up GPRS
[[./img/gprs/gprs_accept.png]]

** Fast GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_0.png]]
** Fast GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_1.png]] 
** Fast GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_2.png]] 
** Fast GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_3.png]] 
** Fast GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_4.png]] 
** Fast GPRS with $P_X = \mathcal{N}(0, 1), P_{X \mid Y} = \mathcal{N}(1, 1/16)$
[[./img/fast_gprs/fast_gprs_5.png]]

** Analysis of faster GPRS
#+ATTR_REVEAL: :frag (appear)
Now, encode search path $\pi$.

#+ATTR_REVEAL: :frag (appear)
$H[\pi] \leq I[X; Z] + \log(I[X; Z] + 1) + \mathcal{O}(1)$

#+ATTR_REVEAL: :frag (appear)
$\mathbb{E}[\lvert\pi\rvert] = \mathcal{O}(I[X; Z])$

* References
** References I
- E. Agustsson and L. Theis. "Universally quantized neural compression" In NeurIPS 2020.
- C. Blundell, J. Cornebise, K. Kavukcuoglu and D. Wierstra. Weight uncertainty in neural network. In ICML 2015.
- E. Dupont, A. Golinski, M. Alizadeh, Y. W. Teh and Arnaud Doucet. "COIN: compression with implicit neural representations" arXiv preprint arXiv:2103.03123, 2021.

** References II
- G. F. “Greedy Poisson Rejection Sampling” NeurIPS 2023, to appear.
- G. F.*, S. Markou*, and J. M. Hernandez-Lobato. "Fast relative entropy coding with A* coding". In ICML 2022.
- D. Goc and G. F. “On Channel Simulation Conjectures” unpublished.

** References III
- Z. Guo*, G. F.*, J. He, Z. Chen and J. M. Hernandez Lobato, “Compression with Bayesian Implicit Neural Representations” NeurIPS 2023, to appear.
- P. Harsha, R. Jain, D. McAllester, and J. Radhakrishnan, “The communication complexity of correlation,” IEEE Transactions on Information Theory, vol. 56, no. 1, pp. 438–449, 2010.
- M. Havasi, R. Peharz, and J. M. Hernández-Lobato. "Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters" In ICLR 2019.

** References IV
- J. He*, G. F.*, Z. Guo and J. M. Hernandez Lobato, “RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations” unpublished.
- C. T. Li and A. El Gamal, “Strong functional representation lemma and applications to coding theorems,” IEEE Transactions on Information Theory, vol. 64, no. 11, pp. 6967–6978, 2018.

** References V
- L. Theis and E. Agustsson. On the advantages of stochastic encoders. arXiv preprint arXiv:2102.09270.
- L. Theis, T. Salimans, M. D. Hoffman and F. Mentzer (2022). Lossy compression with Gaussian diffusion. arXiv preprint arXiv:2206.08889.

